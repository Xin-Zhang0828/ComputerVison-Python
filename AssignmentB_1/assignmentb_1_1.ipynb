{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MNIST classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main references:\n",
    "\n",
    "    Andrew Ng, Machine Learning, https://www.coursera.org/learn/machine-learning\n",
    "    https://zhuanlan.zhihu.com/p/38709373\n",
    "    https://github.com/alcarasj/simple-vision-stuff/blob/master/assignment2/main.py       https://github.com/bdura/humanware/blob/469c0a9a285b83f0951b6d88608613047894197f/models/CNN.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. In the file mnist.py identify where the Stochastic Gradient Descent optimizer is created. Train the default CNN architecture by choosing appropriate parameter values for:\n",
    "\n",
    "     i) Learning rate – explain what happens when you use too large or too small value and explain why it is happening.\n",
    "\n",
    "     ii) Weight decay – explain what happens when you use too large or too small regularization and explain why it is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer：\n",
    "\n",
    "     i) In the process of  training, we can adjust the network weights by controlling the updates rate. If the learning rate is too small, the loss function changes slowly and it will spend more time to converge, so it is easy to produce over-fitting; On the other hand, when the learning rate is relatively large, the learning speed will be faster, but the gradient decent can overshoot the minimum so it may fail to converge or even diverge. Therefore, the value of the learning rate should be moderate.\n",
    "     \n",
    "     ii) In the loss function, weight decay is a coefficient placed in front of the regularization. The regular term generally indicates the complexity of the model. Therefore, the effect of weight decay is to adjust the influence of model complexity on the loss function. The ultimate goal is to prevent Overfitting. If the weight decay is large, the value of the complex model loss function is large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.01# TODO\n",
    "momentum = 0.9\n",
    "weight_decay = 0.04 # TODO\n",
    "log_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, \n",
    "                               kernel_size=5, stride=1, padding=0)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, \n",
    "                               kernel_size=3, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(in_features=800, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    " \n",
    "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='linear')\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, 800)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Correct the mistakes in CNN2 and train it on MNIST train set. Desired architecture of CNN2 is\n",
    "displayed on following diagam:\n",
    "![title](img/con.png)\n",
    "CONV KxK, N represents N features extracted by KxK filters, FC N represent fully-connected layer with N nodes. Set the padding so each convolution preserves input feature size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, \n",
    "                               kernel_size=5, stride=1, padding=2)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, \n",
    "                               kernel_size=5, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(in_features=3136, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=10)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, 3136)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Design CNN3 with additional regularization of your choosing. Explain benefits of such regularization and report its accurracy on fashion MNIST and its relative improvement over CNN2.\n",
    "\n",
    "Answer：\n",
    "\n",
    "    The accuracy rate has improved two times compared with the previous one although the end result is not particularly high. \n",
    "    1）After the convolutional layer of the convolutional neural network, BatchNorm2d is  added to normalize the data, which makes the data not unstable due to excessive data before the Relu is performed.\n",
    "    2）Modified the stride of the last layer of maxpool so that the downsampling is not twice as high.\n",
    "    3）Dropout reduces the dependencies between nodes, selectively ignoring individual neurons and avoiding overfitting. The entire dropout process is equivalent to averaging many different neural networks. Different networks produce different over-fittings, and some “reverse” fittings reduce the over-fitting as a whole. Besides, the dropout causes two neurons not necessarily to appear in one dropout network each time. Such updates of weights will no longer rely on the interaction of implicit nodes with fixed relationships, preventing situations where certain features are only effective under other specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.05)\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )   \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(in_features=18432, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=.05)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.max = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=9216, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.conv4.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = x.size(0)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.max(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Flatten the tensor for dense layers\n",
    "\n",
    "        x = x.view(-1,9216)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, label, text):\n",
    "    fig = plt.figure()\n",
    "    for i in range(6):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(data[i][0], cmap='gray', interpolation='none')\n",
    "        plt.title(text + \": {}\".format(label[i]))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, device, test_loader):\n",
    "    examples = enumerate(test_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_idx, (data, target) = next(examples)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.cpu().data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        pred = pred.numpy()\n",
    "    return data.cpu().data.numpy(), target.cpu().data.numpy(), pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(train_x, train_y, test_x, test_y, ylabel=''):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_x, train_y, color='blue')\n",
    "    plt.plot(test_x, test_y, color='red')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.xlabel('number of training examples seen')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, losses=[], counter=[], errors=[]):\n",
    "    model.train()\n",
    "    correct=0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            losses.append(loss.item())\n",
    "            counter.append((batch_idx*batch_size) + ((epoch-1)*len(train_loader.dataset)))\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    errors.append(100. * (1 - correct / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, losses=[], errors=[]):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    losses.append(test_loss)\n",
    "    errors.append(100. *  (1 - correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Change dataset to fashion MNIST (https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/, hint: take a look at torchvision.datasets), estimate the dataset mean and standard deviation and use it to normalize the data in the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # data transformation\n",
    "    train_data = datasets.FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "    test_data = datasets.FashionMNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "    # data loaders\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # extract and plot random samples of data\n",
    "    #examples = enumerate(test_loader)\n",
    "    #batch_idx, (data, target) = next(examples)\n",
    "    #plot_data(data, target, 'Ground truth')\n",
    "\n",
    "    # model creation\n",
    "    model = CNN3().to(device)\n",
    "    # optimizer creation\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0, momentum=momentum, weight_decay=0)\n",
    "\n",
    "    # lists for saving history\n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    test_losses = []\n",
    "    test_counter = [i*len(train_loader.dataset) for i in range(epochs + 1)]\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    error_counter = [i*len(train_loader.dataset) for i in range(epochs)]\n",
    "\n",
    "    # test of randomly initialized model\n",
    "    test(model, device, test_loader, losses=test_losses)\n",
    "\n",
    "    # global training and testing loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, losses=train_losses, counter=train_counter, errors=train_errors)\n",
    "        test(model, device, test_loader, losses=test_losses, errors=test_errors)\n",
    "\n",
    "    # plotting training history\n",
    "    plot_graph(train_counter, train_losses, test_counter, test_losses, ylabel='negative log likelihood loss')\n",
    "    plot_graph(error_counter, train_errors, error_counter, test_errors, ylabel='error (%)')\n",
    "\n",
    "    # extract and plot random samples of data with predicted labels\n",
    "    data, _, pred = predict_batch(model, device, test_loader)\n",
    "    plot_data(data, pred, 'Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3029, Accuracy: 1778/10000 (18%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.769168\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.892540\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.664487\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.612219\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.592206\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.632141\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.803847\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.595777\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.655798\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.761323\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.388552\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.669578\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.720459\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.641393\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.765249\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.675633\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.577637\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.664344\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.783052\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.621409\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.768837\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.783540\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.753870\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.607236\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.655555\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.582439\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.701548\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.693254\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.630331\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.492199\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.614604\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.458900\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.680519\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.751732\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.786924\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.720566\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.605455\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.629763\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.660820\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.415458\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.720985\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 2.513422\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 2.631800\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 2.571200\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 2.738673\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.624743\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 2.674231\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
